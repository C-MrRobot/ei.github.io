{"meta":{"title":"Bigdata大数据","subtitle":"","description":"","author":"MrRobot.C","url":"https://c-mrrobot.github.io/ei.github.io","root":"/ei.github.io/"},"pages":[],"posts":[{"title":"Hexo部署在GitHub Pages指导","slug":"Hexo部署在GitHub-Pages指导","date":"2021-11-17T23:33:59.000Z","updated":"2021-11-19T14:56:15.235Z","comments":true,"path":"2021/11/18/Hexo部署在GitHub-Pages指导/","link":"","permalink":"https://c-mrrobot.github.io/ei.github.io/2021/11/18/Hexo%E9%83%A8%E7%BD%B2%E5%9C%A8GitHub-Pages%E6%8C%87%E5%AF%BC/","excerpt":"","text":"hexo部署在github pages指导1. 本地安装Hexo环境 (Linux环境)12345$apt install nodejs-lts$npm install -g hexo-cli$npm install hexo-deployer-git$npm g$npm d 2. 本地准备部署工具1$apk install openssh git vim 3. 生成ssh公钥1$ssh-keygen -t rsa -C &quot;user.email&quot; 4. Github环境准备设置中先添加ssh公钥创建github仓库 命名:username.github.io克隆仓库到本地目录复制本地blog目录下的文件到仓库中修改:’_config.yml’ 文件，末尾添加如下信息: deploy: type: git repo: &#x67;&#105;&#x74;&#64;&#103;&#105;&#x74;&#x68;&#x75;&#98;&#x2e;&#x63;&#111;&#109;:C-MrRobot/ei.github.io.git branch: main (注意:github默认是main分支) 在clone下来的仓库中添加插件，配置githubconfig1234$npm install hexo-deployer-git$git config --global core.autocrlf false$git config --global user.email &quot;useremail&quot;$git config --global user.name &quot;username&quot; 5. 推送代码到github123$hexo clean$hexo g $hexo d 6. 前往github 仓库中的设置配置git pages页面，选择要显示的分支保存即可注意:仓库属性为public 公开的才能提供页面服务，private 私有仓库不能显示，每改变一次仓库属性，需要重新配置git pages。","categories":[],"tags":[]},{"title":"Hive从入门到精通","slug":"Hive基础-由浅入深","date":"2021-11-17T15:23:17.000Z","updated":"2021-11-19T15:35:53.695Z","comments":true,"path":"2021/11/17/Hive基础-由浅入深/","link":"","permalink":"https://c-mrrobot.github.io/ei.github.io/2021/11/17/Hive%E5%9F%BA%E7%A1%80-%E7%94%B1%E6%B5%85%E5%85%A5%E6%B7%B1/","excerpt":"","text":"Hive常用操作beeline客户端命令：123456$beeline --hiveconf hive.server2.logging.operation.level=NONE$beeline -f &quot;test.sql&quot;$beeline -e show databases$beeline -n username -p password -u jdbc:hive2://hs2.local:10012$beeline -u jdbc:hive2://192.168.0.76:10000 -n postgres$beeline --hiveconf &#x27;hive.server2.tez.initialize.default.sessions=true&#x27; &#x27;hive.cli.print.header=false&#x27; -e &#x27;select * from table&#x27; 123456&gt;describe formated table_name;&gt;show create database $dbname;&gt;show create table $tablename;&gt;drop database myhive cascade; &gt;TRUNCATE TABLE Tablename;&gt;describe database bigdata; Hive常用语法修改执行引擎:1&gt;set hive.execution.engine=spark; 修改hive 提交队列：1&gt;set mapreduce.job.queuename=default 修改hive表的ower：1&gt;ALTER TABLE $table_name SET OWNER user $username; 修改表的序列化1&gt;ALTER TABLE $tablename set SERDE &quot;org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe&quot;; (RCFlie) 修改表存储格式：123&gt;ALTER TABLE $tablename SET FILEFORMATINPUTFORMAT “org.apache.hadoop.hive.ql.io.RCFileInputFormat”OUTPUTFORMAT “org.apache.hadoop.hive.ql.io.RCFileOutputFormat”; 修改存储路径：https://www.cnblogs.com/weijiqian/p/14335772.html 1&gt;ALTER TABLE $table_name set location &#x27;/hive/warehouse/test.db/tb_table2&#x27;; Hive metastore元数据操作12$find / -name gsql$./gsql -p 20051 -d hivemeta -U omm -W dbserverAdmin@123 查询表结构：1&gt;select pg_get_tabledef(&#x27;public.HUE_PERMISSION_id_SEQ&#x27;) hive默认元数据为DBserver服务。omm 管理员密码：dbserverAdmin@123hive 密码：HiveUser@ 如果被锁需解锁，语句：ALTER USER hive ACCOUNT UNLOCKHive调优性能优化：https://support.huaweicloud.com/cmpntguide-mrs/mrs_01_0979.html 1&gt;set spark.sql.hive.convertMetastoreParquet = false; spark自带的序列化方式无法解析hive中的parquet数据，便读不到数据，此时可以将这个参数设置为false。 yarn yarn.nodemanager.resource.cpu-vcores 调整每个nodemanager占用core节点的cpu核数 yarn.nodemanager.resource.memory-mb 调整每个nodemanager占用core节点的memory数 yarn.scheduler.minimum-allocation-mb 每个contain最小申请的内存 yarn.scheduler.maximum-allocation-mb 每个contain最大申请的内存 yarn.scheduler.minimum-allocation-vcores 每个contain最小申请的核数 yarn.scheduler.maximum-allocation-vcores 每个contain最大申请的核数 mapreduce.map.memory.mb： 每个map任务内存量 mapreduce.map.cpu.vcores 每个map任务cpu数量 MR引擎 set mapred.min.split.size=1; set mapred.max.split.size=128000000; 增大map task的数量 set mapred.map.tasks=2； 增大map task的数量 ，无用！！ set mapred.reduce.tasks=1000; 增大reduce task的数量 mapreduce.map.memory.mb 每个Map task设置map的最大内存 mapreduce.reduce.memory.mb 每个Reduce task设置reduce的最大内存 TEZ引擎 1.set hive.tez.cpu.vcores=3 设置每个task的cpu使用 2.如果tez是contain模式，尽量调节contain的数量，因为contain多了，那么里面运行的task也多了，并行度就上来了 spark引擎1&gt;set hive.execution.engine=spark; Hive实战1· 创建内表12345&gt;create table bigdata.eimrs(id string,name string,worker string,happies string,age int);&gt;insert into table bigdata.eimrs values(&quot;0&quot;,&quot;张三&quot;,&quot;程序员&quot;,&quot;踢足球&quot;,30);&gt;insert into table bigdata.eimrs values(&quot;1&quot;,&quot;李四&quot;,&quot;艺术家&quot;,&quot;打篮球&quot;,28);&gt;insert into table bigdata.eimrs values(&quot;2&quot;,&quot;王五&quot;,&quot;程序员&quot;,&quot;乒乓球&quot;,32);&gt;insert into table bigdata.eimrs values(&quot;3&quot;,&quot;成哥&quot;,&quot;程序员&quot;,&quot;铅球&quot;,32); 2· 创建外部表12345&gt;create external table student (s_id string,s_name string) row format delimited fields terminated by &#x27;\\t&#x27;;&gt;create external table bigdata.eimrs0(id string comment &quot;id&quot;,name string comment &quot;名称&quot;,teacher string comment &quot;授课老师信息&quot;)comment &quot;测试JSON格式外部表&quot;row format serde &#x27;org.apache.hive.hcatalog.data.JsonSerDe&#x27;stored as textfile; 3· 创建分区表12&gt;create table score2 (s_id string, s_score int) partitioned by (year string,month string,day string);&gt;insert into table score2 partition(year=&#x27;2021&#x27;,month=&#x27;01&#x27;,day=&#x27;01&#x27;) values(&quot;1&quot;,20); 4· 创建表指定obs 路径12&gt;create database if not exists $database_name location &quot;obs://桶/目录/&quot;;&gt;create table test(name string) location &quot;obs://桶/user/hive/warehouse/test&quot;; 5. load本地数据追加操作 123&gt;load data local inpath &#x27;/export/servers/hivedatas/student.csv&#x27; into table student;&gt;load data local inpath &#x27;/export/servers/hivedatas/student.csv&#x27; overwrite into table student; 6. loadHDFS数据12&gt;load data inpath &#x27;/hivedatas/techer.csv&#x27; into table techer;&gt;load data inpath &#x27;/hivedatas/techer.csv&#x27; into table techer partition(cur_date=20201210); （load到指定分区） 7. 修改hive表的ower1&gt;ALTER TABLE $table_name SET OWNER user $username; 8. 修改存储路径https://www.cnblogs.com/weijiqian/p/14335772.html 1&gt;ALTER TABLE $table_name SET location &#x27;/hive/warehouse/test.db/tb_table2&#x27;;","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2021-11-17T13:54:34.971Z","updated":"2021-11-17T13:54:34.971Z","comments":true,"path":"2021/11/17/hello-world/","link":"","permalink":"https://c-mrrobot.github.io/ei.github.io/2021/11/17/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[],"tags":[]}